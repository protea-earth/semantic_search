{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import requests\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4.element import Comment\n",
    "from urllib.error import HTTPError\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = ['https://www.drawdown.org/', 'https://www.c40.org/',  'https://socialcapitalmarkets.net/',  'https://www.microsoft.com/en-us/ai/ai-for-earth-grants', \n",
    "         'https://youngstartup.com/west2019/?utm_source=IPs&utm_campaign=ClimateLink', 'https://www.acterra.org/',\n",
    "         'https://www.meetup.com/ClimateLink-SanFrancisco/events/260257852/', 'https://meetingoftheminds.org/',\n",
    "        'https://wearesocial.com/global-digital-report-2019',\n",
    "'https://www.statista.com/statistics/272014/global-social-networks-ranked-by-number-of-users/',\n",
    "'https://www.statista.com/chart/17613/most-popular-websites/',\n",
    "'http://digg.com/2019/most-visited-websites-world',\n",
    "'https://www.weforum.org/agenda/2017/03/most-popular-social-networks-mapped/',\n",
    "'https://vincos.it/world-map-of-social-networks/',\n",
    "'https://www.pewinternet.org/fact-sheet/social-media/',\n",
    "'https://knowledge.wharton.upenn.edu/article/impact-of-social-media/',\n",
    "'https://www.fastcompany.com/90281575/social-media-predictions-for-2019-a-return-to-personal-authenticity',\n",
    "'https://www.knightfoundation.org/funding-opp-research-norms-rules-governance-internet-digital-platforms',\n",
    "'https://www.forbes.com/sites/ryanholmes/2018/10/29/are-facebook-groups-the-future-of-social-media-or-a-dead-end/',\n",
    "'https://blog.hootsuite.com/social-media-trends/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = open(\"Links.txt\",\"r\") #download google doc as .txt file to get Links.txt\n",
    "with open('Links.txt', 'r', encoding=\"utf8\") as file:\n",
    "    data = file.read().replace('\\n', ' ')\n",
    "def Find(data):  \n",
    "    return re.findall(r'(https?://[^\\s]+)', data)\n",
    "sites = sites + Find(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "headers = {'User-Agent':'Chrome/78.0'} #change this based on your browser. \n",
    "#check here https://www.whatismybrowser.com/detect/what-is-my-user-agent\n",
    "filter_words = ['http', 'www']\n",
    "html_corpus = [] \n",
    "scraped = []\n",
    "errors = 0\n",
    "for base in sites:\n",
    "    #this section before the next for loop gets html from base site before looping over all links on base site\n",
    "    req = Request(base, headers=headers)\n",
    "    try: #error handling to catch dead links, etc., and prevent code from stopping for that   \n",
    "        page = urlopen(req)\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        html_corpus.append(soup.prettify())\n",
    "        scraped.append(base)\n",
    "    except Exception as e:\n",
    "        #print('error ' + base) uncomment this to help with debugging\n",
    "        errors = errors + 1\n",
    "        #continue\n",
    "    for link in soup.findAll('a'): #iterate over all links on base site\n",
    "        if link.has_attr('href') and not any(x in link['href'] for x in filter_words) and len(link['href']) > 0:\n",
    "            #line above prevents scraping external link\n",
    "            if base[len(base) - 1] == '/' and link['href'][0] == '/': #double slash between base url and href \n",
    "                #can lead to invalid urls so I remove extra one here \n",
    "                site = base + link['href'][1:]\n",
    "            else:\n",
    "                site = base + link['href']\n",
    "            if site not in scraped: #makes sure site isn't scraped twice\n",
    "                #print(site) uncomment this to help with debugging\n",
    "                linkreq = Request(site,headers=headers)\n",
    "                try:\n",
    "                    linkpage = urlopen(linkreq)\n",
    "                    linksoup = BeautifulSoup(linkpage, 'html.parser')\n",
    "                    html_corpus.append(linksoup.prettify())\n",
    "                    scraped.append(site)\n",
    "                except Exception as e:\n",
    "                    #print('error ' + site) uncomment this to help with debugging\n",
    "                    errors = errors + 1\n",
    "                    #continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(list(zip(scraped, html_corpus)), \n",
    "               columns =['Site', 'HTML']) \n",
    "data.to_csv('html.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To get text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent':'Chrome/78.0'} #change this based on your browser. \n",
    "#check here https://www.whatismybrowser.com/detect/what-is-my-user-agent\n",
    "filter_words = ['http', 'www']\n",
    "scraped  = []\n",
    "text_corpus = [] #this will be the same length as the sites list\n",
    "errors = 0\n",
    "for base in sites:\n",
    "    #print(\"BASE \" + base) uncomment this to help with debugging\n",
    "    #this section before the next for loop gets html from base site before looping over all links on base site\n",
    "    try: #error handling to catch dead links, etc., and prevent code from stopping for that\n",
    "        html = urllib.request.urlopen(base).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        text_corpus.append(text_from_html(html))\n",
    "        scraped.append(base)\n",
    "    except Exception as e:\n",
    "        #print('error ' + base) uncomment this to help with debugging\n",
    "        errors = errors + 1\n",
    "    for link in soup.findAll('a'): #iterate over all links on base site\n",
    "        if link.has_attr('href') and not any(x in link['href'] for x in filter_words) and len(link['href']) > 0:\n",
    "            #line above prevents scraping external link\n",
    "            if base[len(base) - 1] == '/' and link['href'][0] == '/': #double slash between base url and href \n",
    "                #can lead to invalid urls so I remove extra one here \n",
    "                site = base + link['href'][1:]\n",
    "            else:\n",
    "                site = base + link['href']\n",
    "            if site not in scraped: #makes sure site isn't scraped twice\n",
    "                #print(site) uncomment this to help with debugging\n",
    "                try:\n",
    "                    html_link = urllib.request.urlopen(site).read()\n",
    "                    text_corpus.append(text_from_html(html_link))\n",
    "                    scraped.append(site)\n",
    "                except Exception as e:\n",
    "                    #print('error ' + site) uncomment this to help with debugging\n",
    "                    errors = errors + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
